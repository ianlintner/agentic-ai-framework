# LLM Integration Implementation Details

This document outlines the implementation details for the Large Language Model (LLM) integrations in the Agentic AI Framework, focusing on the Claude integration via Google Vertex AI.

## Overview

The Agentic AI Framework now provides comprehensive integration with Anthropic's Claude models via Google Vertex AI, including:

- Streaming support for real-time responses
- Rate limiting and quota management
- Memory-based context management
- Conversation persistence
- Comprehensive test coverage

## Vertex AI Client

The `VertexAIClient` class provides a clean, ZIO-based interface to Google's Vertex AI platform, which is used to access Claude and other models.

### Key Features

1. **True Streaming Support**
   - Implements token-by-token streaming for real-time responses
   - Uses Vertex AI's streaming API instead of simulating streaming by splitting a complete response

2. **Rate Limiting and Quota Management**
   - Configurable request interval to avoid rate limiting
   - Retry schedules for handling transient errors
   - Quota awareness to prevent service disruptions

3. **Configuration Options**
   - Flexible configuration for different use cases (standard, high-throughput, low-latency)
   - Environment variable support for easier deployment across environments
   - Model-specific presets for common LLMs

### Implementation Notes

- Uses the Vertex AI Generative API for streaming responses
- Provides both synchronous `complete()` and streaming `streamCompletion()` methods
- Implements a `ZLayer` for easy integration with ZIO applications

## Claude Agent

The `ClaudeAgent` class builds on the Vertex AI client to provide a memory-integrated agent that can maintain conversation context.

### Key Features

1. **Memory-Based Context Management**
   - Stores conversation history in the memory system
   - Retrieves relevant context for new prompts
   - Maintains continuity across multiple interactions

2. **Conversation Persistence**
   - Automatically saves both user messages and assistant responses
   - Tags conversations for easy retrieval
   - Supports conversation history truncation to stay within context limits

3. **Context Management**
   - Allows storing and retrieving additional context information
   - Supports conversation clearing to start fresh

### Implementation Notes

- Integrates with the memory system using the `MemorySystem` trait
- Formats prompts according to Claude's expected format
- Provides factory methods for easy agent creation

## Memory Integration

The LLM components integrate with the framework's memory system in several ways:

1. **Conversation Storage**
   - User messages and assistant responses are stored as `ConversationTurn` objects
   - Each turn is tagged for easy retrieval
   - Timestamps enable chronological ordering

2. **Context Management**
   - Separate context storage from conversation history
   - Tagged memory cells for efficient retrieval
   - Support for arbitrary context data

3. **History Management**
   - Automatic truncation to manage context length
   - Configurable history length limits
   - Support for complete history clearing

## Testing Approach

The implementation includes comprehensive testing:

1. **Mock Testing**
   - Mock client for testing without API dependencies
   - Mock memory system for isolated testing
   - Throttle shape for simulating streaming responses

2. **Configuration Testing**
   - Test different configurations behave as expected
   - Verify rate limiting configurations work correctly
   - Test factory methods for proper initialization

3. **Memory Integration Testing**
   - Test conversation storage and retrieval
   - Verify context management functions
   - Test history truncation and clearing

4. **Live API Testing (Optional)**
   - Configurable live API tests for integration verification
   - Environment variable control to enable/disable live tests
   - Simple prompts to minimize token usage during testing

## Usage Example

```scala
// Create a Claude agent
val program = for {
  agent <- ClaudeAgent.makeDefault("my-assistant")
  
  // Add some context
  _ <- agent.addContext("user_name", "John")
  
  // Process a message and get streaming response
  response <- agent.process("Hello, what's the weather like today?").runCollect
  
  // Next message will have context of previous interaction
  followUp <- agent.process("Do I need an umbrella?").runCollect
} yield (response, followUp)

// Run with provided dependencies
val result = program.provide(
  MemorySystem.live,
  VertexAIClient.live
)
```

## Configuration Example

```scala
// Standard configuration
val standardConfig = VertexAIConfig.claudeDefault.copy(
  projectId = "my-gcp-project",
  location = "us-central1"
)

// Low latency configuration for chat applications
val chatConfig = VertexAIConfig.lowLatency.copy(
  projectId = "my-gcp-project",
  location = "us-central1"
)

// High throughput configuration for batch processing
val batchConfig = VertexAIConfig.highThroughput.copy(
  projectId = "my-gcp-project",
  location = "us-central1",
  quotaLimit = Some(400) // Custom quota limit
)
```

## Future Enhancements

While the current implementation meets all the requirements in the roadmap, several future enhancements could be considered:

1. **Prompt Templates**
   - Add support for prompt templates to standardize agent interactions
   - Enable system prompts for agent personality and capabilities

2. **Tool Usage**
   - Implement function calling capabilities
   - Enable agents to use external tools and APIs

3. **Embeddings Support**
   - Add support for generating embeddings for RAG applications
   - Integrate with vector search capabilities

4. **Multi-Model Support**
   - Expand configuration options for other models available on Vertex AI
   - Add model fallbacks for resilience

5. **Advanced Caching**
   - Implement response caching for efficiency
   - Support for deterministic output with the same input
